{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tugas Lab 4 - Fix NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKTJhNm0EP2D"
      },
      "source": [
        "# Tugas Lab 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFcUswHEUDi",
        "outputId": "d1404051-dee0-4678-cafb-aee02277884e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "print('-'*42+'\\nTugas Lab 4 PengCit - Kelompok Apa\\n'+'-'*42)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------\n",
            "Tugas Lab 4 PengCit - Kelompok Apa\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ROEhM9kwwk"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEUU6OYFjLvS"
      },
      "source": [
        "Color : R, G, B, S\n",
        "\n",
        "Texture : Contrast, ASM, Homogeneity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J18TCS5qj7ms"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys, math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6uohnwVLTJS"
      },
      "source": [
        "# For google colab purpose\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZAMBnRIkLqK"
      },
      "source": [
        "import glob\n",
        "from skimage import io, color, util, feature\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Ubah sesuai kebutuhan\n",
        "workdir = '/content/drive/My Drive/TL4 PengCit/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESq0VYlddJjN"
      },
      "source": [
        "## Feature Model 1 : RGB + S"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF9juJXZj9Pd"
      },
      "source": [
        "train_shape_feature = [[\"avg_r\", \"avg_g\", \"avg_b\", \"avg_s\", \"type_label\"]]\n",
        "\n",
        "# CUACA TRAIN FEATURE MODEL 1\n",
        "def generate_train(type_str, type_label):\n",
        "  files_tr_clear = glob.glob(workdir + \"Cuaca/Train/{}/**/*\".format(type_str))\n",
        "  N = len(files_tr_clear)\n",
        "\n",
        "  for k in range(N):\n",
        "    RGB = io.imread(files_tr_clear[k])\n",
        "    R = RGB[:,:,0]\n",
        "    G = RGB[:,:,1]\n",
        "    B = RGB[:,:,2]\n",
        "    avg_r = np.average(R.flatten())\n",
        "    avg_g = np.average(G.flatten())\n",
        "    avg_b = np.average(B.flatten())\n",
        "\n",
        "    HSV = color.rgb2hsv(RGB)\n",
        "    S = HSV[:,:,1]\n",
        "    avg_s = np.average(S.flatten())\n",
        "\n",
        "    train_shape_feature.append([avg_r, avg_g, avg_b, avg_s, type_label])\n",
        "\n",
        "generate_train(\"Clear\", 0)\n",
        "generate_train(\"Haze\", 1)\n",
        "generate_train(\"Underwater\", 2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0lgkvUUkFvh"
      },
      "source": [
        "# CREATE TRAIN CSV FEATURE MODEL 1\n",
        "train_shape_feature = np.array(train_shape_feature)\n",
        "np.savetxt(workdir + 'train_shape_feature_1.csv', train_shape_feature, delimiter=',', fmt='%s')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2s1nSamM9ut"
      },
      "source": [
        "test_shape_feature = [[\"avg_r\", \"avg_g\", \"avg_b\", \"avg_s\", \"type_label\"]]\n",
        "\n",
        "# CUACA TEST FEATURE MODEL 1\n",
        "def generate_test(type_str, type_label):\n",
        "  files_tr_clear = glob.glob(workdir + \"Cuaca/Test/{}/**/*\".format(type_str))\n",
        "  N = len(files_tr_clear)\n",
        "\n",
        "  for k in range(N):\n",
        "    RGB = io.imread(files_tr_clear[k])\n",
        "    R = RGB[:,:,0]\n",
        "    G = RGB[:,:,1]\n",
        "    B = RGB[:,:,2]\n",
        "    avg_r = np.average(R.flatten())\n",
        "    avg_g = np.average(G.flatten())\n",
        "    avg_b = np.average(B.flatten())\n",
        "\n",
        "    HSV = color.rgb2hsv(RGB)\n",
        "    S = HSV[:,:,1]\n",
        "    avg_s = np.average(S.flatten())\n",
        "\n",
        "    test_shape_feature.append([avg_r, avg_g, avg_b, avg_s, type_label])\n",
        "\n",
        "generate_test(\"Clear\", 0)\n",
        "generate_test(\"Haze\", 1)\n",
        "generate_test(\"Underwater\", 2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ULVVyJNLFU"
      },
      "source": [
        "# CREATE TEST CSV FEATURE MODEL 1\n",
        "test_shape_feature = np.array(test_shape_feature)\n",
        "np.savetxt(workdir + 'test_shape_feature_1.csv', test_shape_feature, delimiter=',', fmt='%s')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU2g8W27dQw8"
      },
      "source": [
        "## Feature Model 2 : Contrast, ASM, Homogeneity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0n8Hhcopruk"
      },
      "source": [
        "train_shape_feature = [[\"contrast\", \"ASM\", \"homogeneity\", \"type_label\"]]\n",
        "\n",
        "# CUACA TRAIN FEATURE MODEL 2\n",
        "def generate_train(type_str, type_label):\n",
        "  files_tr_clear = glob.glob(workdir + \"Cuaca/Train/{}/**/*\".format(type_str))\n",
        "  N = len(files_tr_clear)\n",
        "\n",
        "  for k in range(N):\n",
        "    RGB = io.imread(files_tr_clear[k])\n",
        "    \n",
        "    BW = color.rgb2gray(RGB)\n",
        "    BW = util.img_as_ubyte(BW)\n",
        "    comat = feature.greycomatrix(BW, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                                  levels=256, normed=True, symmetric=True)\n",
        "\n",
        "    contrast = feature.greycoprops(comat, 'contrast')[0, 0]\n",
        "    ASM = feature.greycoprops(comat, 'ASM')[0, 0]\n",
        "    homogeneity = feature.greycoprops(comat, 'homogeneity')[0, 0]\n",
        "\n",
        "    train_shape_feature.append([contrast, ASM, homogeneity, type_label])\n",
        "\n",
        "generate_train(\"Clear\", 0)\n",
        "generate_train(\"Haze\", 1)\n",
        "generate_train(\"Underwater\", 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF8rS-38pu7g"
      },
      "source": [
        "# CREATE TRAIN CSV FEATURE MODEL 2\n",
        "train_shape_feature = np.array(train_shape_feature)\n",
        "np.savetxt(workdir + 'train_shape_feature_2.csv', train_shape_feature, delimiter=',', fmt='%s')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_BLA_K8p4Za"
      },
      "source": [
        "test_shape_feature = [[\"contrast\", \"ASM\", \"homogeneity\", \"type_label\"]]\n",
        "\n",
        "# CUACA TEST FEATURE MODEL 2\n",
        "def generate_test(type_str, type_label):\n",
        "  files_tr_clear = glob.glob(workdir + \"Cuaca/Test/{}/**/*\".format(type_str))\n",
        "  N = len(files_tr_clear)\n",
        "\n",
        "  for k in range(N):\n",
        "    RGB = io.imread(files_tr_clear[k])\n",
        "\n",
        "    BW = color.rgb2gray(RGB)\n",
        "    BW = util.img_as_ubyte(BW)\n",
        "    comat = feature.greycomatrix(BW, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                                  levels=256, normed=True, symmetric=True)\n",
        "\n",
        "    contrast = feature.greycoprops(comat, 'contrast')[0, 0]\n",
        "    ASM = feature.greycoprops(comat, 'ASM')[0, 0]\n",
        "    homogeneity = feature.greycoprops(comat, 'homogeneity')[0, 0]\n",
        "\n",
        "    test_shape_feature.append([contrast, ASM, homogeneity, type_label])\n",
        "\n",
        "generate_test(\"Clear\", 0)\n",
        "generate_test(\"Haze\", 1)\n",
        "generate_test(\"Underwater\", 2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "683YWCZHp8mt"
      },
      "source": [
        "# CREATE TEST CSV FEATURE MODEL 2\n",
        "test_shape_feature = np.array(test_shape_feature)\n",
        "np.savetxt(workdir + 'test_shape_feature_2.csv', test_shape_feature, delimiter=',', fmt='%s')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IFSS1BMdd6e"
      },
      "source": [
        "## PCA Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNMTe1-D9ZjZ"
      },
      "source": [
        "def generate_sq(type_str, subfolder='Train'):\n",
        "  files = glob.glob(workdir + f'Cuaca/{subfolder}/{type_str}/**/*')\n",
        "  \n",
        "  augs = [io.imread(f) for f in files]\n",
        "  sq = []\n",
        "  for i in augs:\n",
        "      h, w = i.shape[0], i.shape[1]\n",
        "      px = min(h, w)\n",
        "      sq.append(i[(h-px)//2:((h-px)//2)+px, (w-px)//2:((w-px)//2)+px])\n",
        "  return sq"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZh3AroUQYDT"
      },
      "source": [
        "square = {\n",
        "    'Clear': generate_sq('Clear'),\n",
        "    'Haze': generate_sq('Haze'),\n",
        "    'Underwater': generate_sq('Underwater'),\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjb_uGnJAOho"
      },
      "source": [
        "minpx = min((d.shape[0] for s in square for d in square[s]))\n",
        "\n",
        "def resizeall(dataset, px):\n",
        "  return [resize(s, (px, px), anti_aliasing=True) for s in dataset]\n",
        "\n",
        "dataset = {k: util.img_as_ubyte(resizeall(square[k], 50)) for k in square}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx1vGD2InNor",
        "outputId": "7a4370fb-acdd-4e8b-f97c-b6aa7caeb57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "dataset.keys()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Clear', 'Haze', 'Underwater'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoLgo2nnnV9t"
      },
      "source": [
        "Dataset final ada di\n",
        "\n",
        "`dataset['Clear']`, `dataset['Haze']`, dan `dataset['Underwater']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3iSyaPWIeyc"
      },
      "source": [
        "# Kode PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOi_Gsd2IpGO"
      },
      "source": [
        "# Input : Matrix\n",
        "# Output : Matrix transformasi linier ke basis yang baru, matriks proyeksi\n",
        "def PCA_proj(X):\n",
        "  cov_mat = np.cov(X.T)\n",
        "\n",
        "  eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "  eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
        "  eig_pairs.sort(key = lambda x : x[0], reverse = True)\n",
        "\n",
        "  exp_var_perc = 0.97\n",
        "  tot = sum(eig_vals)\n",
        "  var_exp = [(i / tot) for i in sorted(eig_vals,  reverse = True)]\n",
        "  cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "  num_vec_to_keep = 0\n",
        "  for i, percentage in enumerate(cum_var_exp):\n",
        "    if percentage > exp_var_perc:\n",
        "      num_vec_to_keep = i + 1\n",
        "      break\n",
        "  \n",
        "  num_features = X.shape[1]\n",
        "  proj_mat = eig_pairs[0][1].reshape(num_features, 1)\n",
        "  for eig_vec_idx in range(1, num_vec_to_keep):\n",
        "    proj_mat = np.hstack((proj_mat, eig_pairs[eig_vec_idx][1].reshape(num_features, 1)))\n",
        "\n",
        "  return proj_mat"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PYdfrzC8hXW"
      },
      "source": [
        "## Feature SVM Processing Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaTuS07n8lvm"
      },
      "source": [
        "from sklearn import svm, metrics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def svm_process(train_file, test_file):\n",
        "  train_df = pd.read_csv(workdir + train_file)\n",
        "  train_df = train_df.sample(frac=1)\n",
        "\n",
        "  train_label = train_df[\"type_label\"]\n",
        "  train_label = pd.DataFrame(train_label).to_numpy()\n",
        "\n",
        "  train_df.drop(labels=\"type_label\", axis=1, inplace=True)\n",
        "  train_shape_feature = pd.DataFrame(train_df).to_numpy()\n",
        "\n",
        "  test_df = pd.read_csv(workdir + test_file)\n",
        "  test_df = test_df.sample(frac=1)\n",
        "\n",
        "  test_label = test_df[\"type_label\"]\n",
        "  test_label = pd.DataFrame(test_label).to_numpy()\n",
        "\n",
        "  test_df.drop(labels=\"type_label\", axis=1, inplace=True)\n",
        "  test_shape_feature = pd.DataFrame(test_df).to_numpy()\n",
        "\n",
        "  train_label = train_label.ravel()\n",
        "  test_label = test_label.ravel()\n",
        "\n",
        "  clf = svm.SVC(gamma='scale')\n",
        "  clf.fit(train_shape_feature, train_label)\n",
        "  svm_label = clf.predict(test_shape_feature)\n",
        "  acc = sum(1 if svm_label[i] == test_label[i] else 0 for i in range(len(test_label))) / len(test_label)\n",
        "  cm_svm = metrics.confusion_matrix(svm_label, test_label)\n",
        "  print(\"Accuracy\", acc)\n",
        "  print(\"Confusion Matrix\")\n",
        "  print(cm_svm)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw_358Cv8tHR",
        "outputId": "29ff813f-6734-426f-bc39-60715bf2fdef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "print(\"Feature 1\")\n",
        "svm_process(\"train_shape_feature_1.csv\", \"test_shape_feature_1.csv\")\n",
        "print()\n",
        "print(\"Feature 2\")\n",
        "svm_process(\"train_shape_feature_2.csv\", \"test_shape_feature_2.csv\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature 1\n",
            "Accuracy 0.7333333333333333\n",
            "Confusion Matrix\n",
            "[[3 2 0]\n",
            " [2 3 0]\n",
            " [0 0 5]]\n",
            "\n",
            "Feature 2\n",
            "Accuracy 0.6666666666666666\n",
            "Confusion Matrix\n",
            "[[5 0 2]\n",
            " [0 5 3]\n",
            " [0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro3Z9tTKhVeb"
      },
      "source": [
        "# PCA SVM Processing Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ora-sIJjBBy7"
      },
      "source": [
        "# Memang lama\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Train Preprocessing\n",
        "merged = np.concatenate((dataset['Clear'], dataset['Haze'], dataset['Underwater']), axis = 0)\n",
        "merged.shape\n",
        "\n",
        "train_label = [0 for i in range(60)] + [1 for i in range(60)] + [2 for i in range(60)]\n",
        "\n",
        "merged = np.array([k.flatten() for k in merged])\n",
        "merged_std = StandardScaler().fit_transform(merged)\n",
        "\n",
        "proj_mat = PCA_proj(merged_std)\n",
        "pca_train = merged_std.dot(proj_mat)\n",
        "pca_train = pca_train.real"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU6ZMKNhkSkI"
      },
      "source": [
        "# Test Preprocessing\n",
        "# testsq = generate_sq('*', 'Test') # di dalam collab, ternyata milihnya random\n",
        "testsq = generate_sq('Clear', 'Test')\n",
        "testsq.extend(generate_sq('Haze', 'Test'))\n",
        "testsq.extend(generate_sq('Underwater', 'Test'))\n",
        "\n",
        "testset = util.img_as_ubyte(resizeall(testsq, 50))\n",
        "testset = np.array([k.flatten() for k in testset])\n",
        "\n",
        "test_label = [0 for i in range(5)] + [1 for i in range(5)] + [2 for i in range(5)]\n",
        "testset_std = StandardScaler().fit_transform(testset)\n",
        "\n",
        "pca_test = testset_std.dot(proj_mat)\n",
        "pca_test = pca_test.real"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfVZL9ePkUmY",
        "outputId": "0336055b-f862-409e-bef2-4c5e4cfef5b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "# SVM\n",
        "train_label = np.array(train_label)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "train_label = train_label.ravel()\n",
        "test_label = test_label.ravel()\n",
        "\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(pca_train, train_label)\n",
        "svm_label = clf.predict(pca_test)\n",
        "\n",
        "acc = sum(1 if svm_label[i] == test_label[i] else 0 for i in range(len(test_label))) / len(test_label)\n",
        "cm_svm = metrics.confusion_matrix(svm_label, test_label)\n",
        "print(\"Accuracy\", acc)\n",
        "print(\"Confusion Matrix\")\n",
        "print(cm_svm)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.8666666666666667\n",
            "Confusion Matrix\n",
            "[[3 0 0]\n",
            " [2 5 0]\n",
            " [0 0 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLcxcJBsujrY"
      },
      "source": [
        "# Improvement\n",
        "Gabung 2 fitur + PCA fiturnya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SxS_xMPm-sq"
      },
      "source": [
        "# Gabung 2 Fitur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvMdxe0avQ44"
      },
      "source": [
        "train_shape_feature = [[\"avg_r\", \"avg_g\", \"avg_b\", \"avg_s\", \"contrast\", \"ASM\", \"homogeneity\", \"type_label\"]]\n",
        "\n",
        "# CUACA TRAIN FEATURE MODEL GABUNGAN\n",
        "def generate_train(type_str, type_label):\n",
        "  files_tr_clear = glob.glob(workdir + \"Cuaca/Train/{}/**/*\".format(type_str))\n",
        "  N = len(files_tr_clear)\n",
        "\n",
        "  for k in range(N):\n",
        "    RGB = io.imread(files_tr_clear[k])\n",
        "    R = RGB[:,:,0]\n",
        "    G = RGB[:,:,1]\n",
        "    B = RGB[:,:,2]\n",
        "    avg_r = np.average(R.flatten())\n",
        "    avg_g = np.average(G.flatten())\n",
        "    avg_b = np.average(B.flatten())\n",
        "\n",
        "    HSV = color.rgb2hsv(RGB)\n",
        "    S = HSV[:,:,1]\n",
        "    avg_s = np.average(S.flatten())\n",
        "\n",
        "    BW = color.rgb2gray(RGB)\n",
        "    BW = util.img_as_ubyte(BW)\n",
        "    comat = feature.greycomatrix(BW, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                                  levels=256, normed=True, symmetric=True)\n",
        "\n",
        "    contrast = feature.greycoprops(comat, 'contrast')[0, 0]\n",
        "    ASM = feature.greycoprops(comat, 'ASM')[0, 0]\n",
        "    homogeneity = feature.greycoprops(comat, 'homogeneity')[0, 0]\n",
        "\n",
        "    train_shape_feature.append([avg_r, avg_g, avg_b, avg_s, contrast, ASM, homogeneity, type_label])\n",
        "\n",
        "generate_train(\"Clear\", 0)\n",
        "generate_train(\"Haze\", 1)\n",
        "generate_train(\"Underwater\", 2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJO1OmIJvQ5M"
      },
      "source": [
        "# CREATE TRAIN CSV FEATURE MODEL GABUNGAN\n",
        "train_shape_feature = np.array(train_shape_feature)\n",
        "np.savetxt(workdir + 'train_shape_feature_3.csv', train_shape_feature, delimiter=',', fmt='%s')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY7E0W2qvQ5R"
      },
      "source": [
        "test_shape_feature = [[\"avg_r\", \"avg_g\", \"avg_b\", \"avg_s\", \"contrast\", \"ASM\", \"homogeneity\", \"type_label\"]]\n",
        "\n",
        "# CUACA TEST FEATURE MODEL GABUNGAN\n",
        "def generate_test(type_str, type_label):\n",
        "  files_tr_clear = glob.glob(workdir + \"Cuaca/Test/{}/**/*\".format(type_str))\n",
        "  N = len(files_tr_clear)\n",
        "\n",
        "  for k in range(N):\n",
        "    RGB = io.imread(files_tr_clear[k])\n",
        "    R = RGB[:,:,0]\n",
        "    G = RGB[:,:,1]\n",
        "    B = RGB[:,:,2]\n",
        "    avg_r = np.average(R.flatten())\n",
        "    avg_g = np.average(G.flatten())\n",
        "    avg_b = np.average(B.flatten())\n",
        "\n",
        "    HSV = color.rgb2hsv(RGB)\n",
        "    S = HSV[:,:,1]\n",
        "    avg_s = np.average(S.flatten())\n",
        "\n",
        "    BW = color.rgb2gray(RGB)\n",
        "    BW = util.img_as_ubyte(BW)\n",
        "    comat = feature.greycomatrix(BW, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                                  levels=256, normed=True, symmetric=True)\n",
        "\n",
        "    contrast = feature.greycoprops(comat, 'contrast')[0, 0]\n",
        "    ASM = feature.greycoprops(comat, 'ASM')[0, 0]\n",
        "    homogeneity = feature.greycoprops(comat, 'homogeneity')[0, 0]\n",
        "\n",
        "    test_shape_feature.append([avg_r, avg_g, avg_b, avg_s, contrast, ASM, homogeneity, type_label])\n",
        "\n",
        "generate_test(\"Clear\", 0)\n",
        "generate_test(\"Haze\", 1)\n",
        "generate_test(\"Underwater\", 2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yBH9A2fvQ5V"
      },
      "source": [
        "# CREATE TEST CSV FEATURE MODEL GABUNGAN\n",
        "test_shape_feature = np.array(test_shape_feature)\n",
        "np.savetxt(workdir + 'test_shape_feature_3.csv', test_shape_feature, delimiter=',', fmt='%s')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErGmx1p9xHDd"
      },
      "source": [
        "train_df = pd.read_csv(workdir + \"train_shape_feature_3.csv\")\n",
        "train_df = train_df.sample(frac=1)\n",
        "\n",
        "train_label = train_df[\"type_label\"]\n",
        "train_label = pd.DataFrame(train_label).to_numpy()\n",
        "\n",
        "train_df.drop(labels=\"type_label\", axis=1, inplace=True)\n",
        "train_shape_feature = pd.DataFrame(train_df).to_numpy()\n",
        "\n",
        "test_df = pd.read_csv(workdir + \"test_shape_feature_3.csv\")\n",
        "test_df = test_df.sample(frac=1)\n",
        "\n",
        "test_label = test_df[\"type_label\"]\n",
        "test_label = pd.DataFrame(test_label).to_numpy()\n",
        "\n",
        "test_df.drop(labels=\"type_label\", axis=1, inplace=True)\n",
        "test_shape_feature = pd.DataFrame(test_df).to_numpy()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRQW6jJZxYEK"
      },
      "source": [
        "from sklearn import svm, metrics\n",
        "\n",
        "train_label = train_label.ravel()\n",
        "test_label = test_label.ravel()\n",
        "\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(train_shape_feature, train_label)\n",
        "svm_label = clf.predict(test_shape_feature)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMXfp_e2xYEf",
        "outputId": "0423c21a-cf07-44c6-e7e3-620e531e57a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "acc = sum(1 if svm_label[i] == test_label[i] else 0 for i in range(len(test_label))) / len(test_label)\n",
        "cm_svm = metrics.confusion_matrix(svm_label, test_label)\n",
        "print(acc)\n",
        "print(cm_svm)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "[[5 0 0]\n",
            " [0 5 0]\n",
            " [0 0 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdRVY5TI5XTM"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VPoJCmU5Z8G",
        "outputId": "b09c75ba-fe9a-49fd-9cb4-1b36301cfa5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_std = StandardScaler().fit_transform(train_shape_feature)\n",
        "proj_mat = PCA_proj(train_std)\n",
        "train_std = train_std.dot(proj_mat)\n",
        "print(train_std.shape)\n",
        "\n",
        "test_std = StandardScaler().fit_transform(test_shape_feature)\n",
        "test_std = test_std.dot(proj_mat)\n",
        "print(test_std.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(180, 5)\n",
            "(15, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haEXguer6spV"
      },
      "source": [
        "from sklearn import svm, metrics\n",
        "\n",
        "train_label = train_label.ravel()\n",
        "test_label = test_label.ravel()\n",
        "\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(train_shape_feature, train_label)\n",
        "svm_label = clf.predict(test_shape_feature)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFHYnDFx7Mgf",
        "outputId": "0c4c3fa0-5fb5-4685-b8c1-0b682ce42f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "acc = sum(1 if svm_label[i] == test_label[i] else 0 for i in range(len(test_label))) / len(test_label)\n",
        "cm_svm = metrics.confusion_matrix(svm_label, test_label)\n",
        "print(acc)\n",
        "print(cm_svm)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "[[5 0 0]\n",
            " [0 5 0]\n",
            " [0 0 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32g6bzsS7ff1"
      },
      "source": [
        "# Cross Validation Hanya Gabung Fitur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z7hdwJZyWJR",
        "outputId": "31f022e2-4eff-499f-c1b6-d0418283178e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "total = 0\n",
        "\n",
        "NUM_LOOP = 10000\n",
        "for i in range(NUM_LOOP):\n",
        "  train_df = pd.read_csv(workdir + \"train_shape_feature_3.csv\")\n",
        "  train_df = train_df.sample(frac=1) # randomized\n",
        "  \n",
        "  N = len(train_df)\n",
        "  n_train = int(N * .8)\n",
        "\n",
        "  train_label = train_df[\"type_label\"][:n_train]\n",
        "  train_label = pd.DataFrame(train_label).to_numpy()\n",
        "\n",
        "  test_label = train_df[\"type_label\"][n_train:]\n",
        "  test_label = pd.DataFrame(test_label).to_numpy()\n",
        "\n",
        "  train_df.drop(labels=\"type_label\", axis=1, inplace=True)\n",
        "  train_shape_feature = pd.DataFrame(train_df[:n_train]).to_numpy()\n",
        "  test_shape_feature = pd.DataFrame(train_df[n_train:]).to_numpy()\n",
        "\n",
        "  train_label = train_label.ravel()\n",
        "  test_label = test_label.ravel()\n",
        "\n",
        "  clf = svm.SVC(gamma='scale')\n",
        "  clf.fit(train_shape_feature, train_label)\n",
        "  svm_label = clf.predict(test_shape_feature)\n",
        "\n",
        "  acc = sum(1 if svm_label[i] == test_label[i] else 0 for i in range(len(test_label))) / len(test_label)\n",
        "  cm_svm = metrics.confusion_matrix(svm_label, test_label)\n",
        "  total += acc\n",
        "\n",
        "print(\"Average Accuracy:\")\n",
        "print(total / NUM_LOOP)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Accuracy:\n",
            "0.8175277777777659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4XARy_r7cT9"
      },
      "source": [
        "# Cross Validation Gabung Fitur + PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i7NWTra7beX",
        "outputId": "cfab09ff-a50a-453d-c0a1-4128342d4636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "total = 0\n",
        "\n",
        "NUM_LOOP = 10000\n",
        "for i in range(NUM_LOOP):\n",
        "  train_df = pd.read_csv(workdir + \"train_shape_feature_3.csv\")\n",
        "  train_df = train_df.sample(frac=1)\n",
        "\n",
        "  N = len(train_df)\n",
        "  n_train = int(N * .8)\n",
        "  n_test = int(N * .2)\n",
        "\n",
        "  train_label = train_df[\"type_label\"][:n_train]\n",
        "  train_label = pd.DataFrame(train_label).to_numpy()\n",
        "\n",
        "  test_label = train_df[\"type_label\"][n_train:]\n",
        "  test_label = pd.DataFrame(test_label).to_numpy()\n",
        "\n",
        "  train_df.drop(labels=\"type_label\", axis=1, inplace=True)\n",
        "  train_shape_feature = pd.DataFrame(train_df[:n_train]).to_numpy()\n",
        "  test_shape_feature = pd.DataFrame(train_df[n_train:]).to_numpy()\n",
        "\n",
        "\n",
        "  train_std = StandardScaler().fit_transform(train_shape_feature)\n",
        "  proj_mat = PCA_proj(train_std)\n",
        "  train_std = train_std.dot(proj_mat)\n",
        "\n",
        "  test_std = StandardScaler().fit_transform(test_shape_feature)\n",
        "  test_std = test_std.dot(proj_mat)\n",
        "  \n",
        "  train_shape_feature = train_std\n",
        "  test_shape_feature = test_std\n",
        "\n",
        "\n",
        "  from sklearn import svm, metrics\n",
        "\n",
        "  train_label = train_label.ravel()\n",
        "  test_label = test_label.ravel()\n",
        "\n",
        "  clf = svm.SVC(gamma='scale')\n",
        "  clf.fit(train_shape_feature, train_label)\n",
        "  svm_label = clf.predict(test_shape_feature)\n",
        "\n",
        "  acc = sum(1 if svm_label[i] == test_label[i] else 0 for i in range(len(test_label))) / len(test_label)\n",
        "  cm_svm = metrics.confusion_matrix(svm_label, test_label)\n",
        "\n",
        "  total += acc\n",
        "\n",
        "print(\"Average Accuracy:\")\n",
        "print(total / NUM_LOOP)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Accuracy:\n",
            "0.891297222222231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEoZxDag_7rQ"
      },
      "source": [
        "# Neural Network with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp3q2B_g__bF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ymjvFzAAw1"
      },
      "source": [
        "classifier = Sequential([\n",
        "    Conv2D(32, (3, 3), input_shape=(50, 50, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(units=128, activation='relu'),\n",
        "    Dense(units=3, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcURGlqNDnjR",
        "outputId": "8c93c6d1-1c04-4153-d95e-fb73aad3aaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator()\n",
        "test_datagen = ImageDataGenerator()\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/TL4 PengCit/Cuaca/Train', target_size=(50, 50), batch_size=1, class_mode='categorical')\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/TL4 PengCit/Cuaca/Test', target_size=(50, 50), batch_size=1, class_mode='categorical')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 180 images belonging to 3 classes.\n",
            "Found 15 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir5oWwX6GkKt",
        "outputId": "5b6e042e-49a4-40d1-c319-4e70576d8b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "classifier.fit(training_set, steps_per_epoch=18, epochs=10, validation_data=test_set, validation_steps=3, verbose=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "18/18 [==============================] - 0s 26ms/step - loss: 887.7383 - accuracy: 0.4444 - val_loss: 189.5140 - val_accuracy: 0.6667\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 39.4285 - accuracy: 0.8333 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 144.6222 - accuracy: 0.6111 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 13.8017 - accuracy: 0.8889 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.7328 - val_accuracy: 0.6667\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 4.6652 - accuracy: 0.8889 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 0s 17ms/step - loss: 1.2053e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 0s 16ms/step - loss: 3.5960e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f946a689cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}